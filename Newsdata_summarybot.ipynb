{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Newsdata_summarybot.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "3bYOoWzjYSip",
        "eNHS_RluhGcv",
        "d3Uzgqx3cLy0",
        "ifDaOjcThpEr",
        "6nhubPG1c5zt"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aZ1lg-rIZJak"
      },
      "source": [
        "#0. Default Setting"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uJkdy_8RZGjc"
      },
      "source": [
        "Executed in Colab environment.\n",
        "\n",
        "* ML Framework\n",
        "   - Python 3.7.10\n",
        "   - Pytorch 1.8.1\n",
        "\n",
        "* Hardware\n",
        "   - RAM: 12.7G \n",
        "   - CPU: Intel(R) Xeon(R) CPU @ 2.20GHz (1core)\n",
        "\n",
        "Assumed that data exists like below.\n",
        "you also need a etri openapi key.\n",
        "\n",
        "```\n",
        "/content/drive/My Drive/\n",
        "├── data\n",
        "│   ├── 1_bert_download_001_bert_morp_pytorch.zip\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3bYOoWzjYSip"
      },
      "source": [
        "#1. Install dependency packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7p9HcD3e8Ryo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aec711e6-5a8b-4b6d-a8b8-7988224e3a32"
      },
      "source": [
        "# install bheinzerling's pyrouge\n",
        "!git clone https://github.com/bheinzerling/pyrouge\n",
        "%cd pyrouge\n",
        "!python setup.py -q install\n",
        "# install missing dependency\n",
        "!apt install -q libxml-parser-perl\n",
        "%cd pyrouge\n",
        "!git clone https://github.com/andersjo/pyrouge.git rouge\n",
        "!pyrouge_set_rouge_path '/content/pyrouge/rouge/tools/ROUGE-1.5.5'\n",
        "%cd /content/pyrouge/rouge/tools/ROUGE-1.5.5/data\n",
        "!mv WordNet-2.0.exc.db WordNet-2.0.exc.db.orig\n",
        "!perl WordNet-2.0-Exceptions/buildExeptionDB.pl ./WordNet-2.0-Exceptions ./smart_common_words.txt ./WordNet-2.0.exc.db"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'pyrouge'...\n",
            "remote: Enumerating objects: 551, done.\u001b[K\n",
            "remote: Total 551 (delta 0), reused 0 (delta 0), pack-reused 551\u001b[K\n",
            "Receiving objects: 100% (551/551), 123.17 KiB | 3.62 MiB/s, done.\n",
            "Resolving deltas: 100% (198/198), done.\n",
            "/content/pyrouge\n",
            "zip_safe flag not set; analyzing archive contents...\n",
            "pyrouge.__pycache__.Rouge155.cpython-37: module references __file__\n",
            "pyrouge.tests.__pycache__.Rouge155_test.cpython-37: module references __file__\n",
            "Reading package lists...\n",
            "Building dependency tree...\n",
            "Reading state information...\n",
            "The following additional packages will be installed:\n",
            "  libauthen-sasl-perl libdata-dump-perl libencode-locale-perl\n",
            "  libfile-listing-perl libfont-afm-perl libhtml-form-perl libhtml-format-perl\n",
            "  libhtml-parser-perl libhtml-tagset-perl libhtml-tree-perl\n",
            "  libhttp-cookies-perl libhttp-daemon-perl libhttp-date-perl\n",
            "  libhttp-message-perl libhttp-negotiate-perl libio-html-perl\n",
            "  libio-socket-ssl-perl liblwp-mediatypes-perl liblwp-protocol-https-perl\n",
            "  libmailtools-perl libnet-http-perl libnet-smtp-ssl-perl libnet-ssleay-perl\n",
            "  libtimedate-perl libtry-tiny-perl liburi-perl libwww-perl\n",
            "  libwww-robotrules-perl netbase perl-openssl-defaults\n",
            "Suggested packages:\n",
            "  libdigest-hmac-perl libgssapi-perl libcrypt-ssleay-perl libauthen-ntlm-perl\n",
            "The following NEW packages will be installed:\n",
            "  libauthen-sasl-perl libdata-dump-perl libencode-locale-perl\n",
            "  libfile-listing-perl libfont-afm-perl libhtml-form-perl libhtml-format-perl\n",
            "  libhtml-parser-perl libhtml-tagset-perl libhtml-tree-perl\n",
            "  libhttp-cookies-perl libhttp-daemon-perl libhttp-date-perl\n",
            "  libhttp-message-perl libhttp-negotiate-perl libio-html-perl\n",
            "  libio-socket-ssl-perl liblwp-mediatypes-perl liblwp-protocol-https-perl\n",
            "  libmailtools-perl libnet-http-perl libnet-smtp-ssl-perl libnet-ssleay-perl\n",
            "  libtimedate-perl libtry-tiny-perl liburi-perl libwww-perl\n",
            "  libwww-robotrules-perl libxml-parser-perl netbase perl-openssl-defaults\n",
            "0 upgraded, 31 newly installed, 0 to remove and 31 not upgraded.\n",
            "Need to get 1,713 kB of archives.\n",
            "After this operation, 5,581 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic/main amd64 netbase all 5.4 [12.7 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu bionic/main amd64 libdata-dump-perl all 1.23-1 [27.0 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu bionic/main amd64 libencode-locale-perl all 1.05-1 [12.3 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu bionic/main amd64 libtimedate-perl all 2.3000-2 [37.5 kB]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu bionic/main amd64 libhttp-date-perl all 6.02-1 [10.4 kB]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu bionic/main amd64 libfile-listing-perl all 6.04-1 [9,774 B]\n",
            "Get:7 http://archive.ubuntu.com/ubuntu bionic/main amd64 libfont-afm-perl all 1.20-2 [13.2 kB]\n",
            "Get:8 http://archive.ubuntu.com/ubuntu bionic/main amd64 libhtml-tagset-perl all 3.20-3 [12.1 kB]\n",
            "Get:9 http://archive.ubuntu.com/ubuntu bionic/main amd64 liburi-perl all 1.73-1 [77.2 kB]\n",
            "Get:10 http://archive.ubuntu.com/ubuntu bionic/main amd64 libhtml-parser-perl amd64 3.72-3build1 [85.9 kB]\n",
            "Get:11 http://archive.ubuntu.com/ubuntu bionic/main amd64 libio-html-perl all 1.001-1 [14.9 kB]\n",
            "Get:12 http://archive.ubuntu.com/ubuntu bionic/main amd64 liblwp-mediatypes-perl all 6.02-1 [21.7 kB]\n",
            "Get:13 http://archive.ubuntu.com/ubuntu bionic/main amd64 libhttp-message-perl all 6.14-1 [72.1 kB]\n",
            "Get:14 http://archive.ubuntu.com/ubuntu bionic/main amd64 libhtml-form-perl all 6.03-1 [23.5 kB]\n",
            "Get:15 http://archive.ubuntu.com/ubuntu bionic/main amd64 libhtml-tree-perl all 5.07-1 [200 kB]\n",
            "Get:16 http://archive.ubuntu.com/ubuntu bionic/main amd64 libhtml-format-perl all 2.12-1 [41.3 kB]\n",
            "Get:17 http://archive.ubuntu.com/ubuntu bionic/main amd64 libhttp-cookies-perl all 6.04-1 [17.2 kB]\n",
            "Get:18 http://archive.ubuntu.com/ubuntu bionic/main amd64 libhttp-daemon-perl all 6.01-1 [17.0 kB]\n",
            "Get:19 http://archive.ubuntu.com/ubuntu bionic/main amd64 libhttp-negotiate-perl all 6.00-2 [13.4 kB]\n",
            "Get:20 http://archive.ubuntu.com/ubuntu bionic/main amd64 perl-openssl-defaults amd64 3build1 [7,012 B]\n",
            "Get:21 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libnet-ssleay-perl amd64 1.84-1ubuntu0.2 [283 kB]\n",
            "Get:22 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libio-socket-ssl-perl all 2.060-3~ubuntu18.04.1 [173 kB]\n",
            "Get:23 http://archive.ubuntu.com/ubuntu bionic/main amd64 libnet-http-perl all 6.17-1 [22.7 kB]\n",
            "Get:24 http://archive.ubuntu.com/ubuntu bionic/main amd64 libtry-tiny-perl all 0.30-1 [20.5 kB]\n",
            "Get:25 http://archive.ubuntu.com/ubuntu bionic/main amd64 libwww-robotrules-perl all 6.01-1 [14.1 kB]\n",
            "Get:26 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libwww-perl all 6.31-1ubuntu0.1 [137 kB]\n",
            "Get:27 http://archive.ubuntu.com/ubuntu bionic/main amd64 liblwp-protocol-https-perl all 6.07-2 [8,284 B]\n",
            "Get:28 http://archive.ubuntu.com/ubuntu bionic/main amd64 libnet-smtp-ssl-perl all 1.04-1 [5,948 B]\n",
            "Get:29 http://archive.ubuntu.com/ubuntu bionic/main amd64 libmailtools-perl all 2.18-1 [74.0 kB]\n",
            "Get:30 http://archive.ubuntu.com/ubuntu bionic/main amd64 libxml-parser-perl amd64 2.44-2build3 [199 kB]\n",
            "Get:31 http://archive.ubuntu.com/ubuntu bionic/main amd64 libauthen-sasl-perl all 2.1600-1 [48.7 kB]\n",
            "Fetched 1,713 kB in 1s (1,899 kB/s)\n",
            "Extracting templates from packages: 100%\n",
            "Selecting previously unselected package netbase.\n",
            "(Reading database ... 160983 files and directories currently installed.)\n",
            "Preparing to unpack .../00-netbase_5.4_all.deb ...\n",
            "Unpacking netbase (5.4) ...\n",
            "Selecting previously unselected package libdata-dump-perl.\n",
            "Preparing to unpack .../01-libdata-dump-perl_1.23-1_all.deb ...\n",
            "Unpacking libdata-dump-perl (1.23-1) ...\n",
            "Selecting previously unselected package libencode-locale-perl.\n",
            "Preparing to unpack .../02-libencode-locale-perl_1.05-1_all.deb ...\n",
            "Unpacking libencode-locale-perl (1.05-1) ...\n",
            "Selecting previously unselected package libtimedate-perl.\n",
            "Preparing to unpack .../03-libtimedate-perl_2.3000-2_all.deb ...\n",
            "Unpacking libtimedate-perl (2.3000-2) ...\n",
            "Selecting previously unselected package libhttp-date-perl.\n",
            "Preparing to unpack .../04-libhttp-date-perl_6.02-1_all.deb ...\n",
            "Unpacking libhttp-date-perl (6.02-1) ...\n",
            "Selecting previously unselected package libfile-listing-perl.\n",
            "Preparing to unpack .../05-libfile-listing-perl_6.04-1_all.deb ...\n",
            "Unpacking libfile-listing-perl (6.04-1) ...\n",
            "Selecting previously unselected package libfont-afm-perl.\n",
            "Preparing to unpack .../06-libfont-afm-perl_1.20-2_all.deb ...\n",
            "Unpacking libfont-afm-perl (1.20-2) ...\n",
            "Selecting previously unselected package libhtml-tagset-perl.\n",
            "Preparing to unpack .../07-libhtml-tagset-perl_3.20-3_all.deb ...\n",
            "Unpacking libhtml-tagset-perl (3.20-3) ...\n",
            "Selecting previously unselected package liburi-perl.\n",
            "Preparing to unpack .../08-liburi-perl_1.73-1_all.deb ...\n",
            "Unpacking liburi-perl (1.73-1) ...\n",
            "Selecting previously unselected package libhtml-parser-perl.\n",
            "Preparing to unpack .../09-libhtml-parser-perl_3.72-3build1_amd64.deb ...\n",
            "Unpacking libhtml-parser-perl (3.72-3build1) ...\n",
            "Selecting previously unselected package libio-html-perl.\n",
            "Preparing to unpack .../10-libio-html-perl_1.001-1_all.deb ...\n",
            "Unpacking libio-html-perl (1.001-1) ...\n",
            "Selecting previously unselected package liblwp-mediatypes-perl.\n",
            "Preparing to unpack .../11-liblwp-mediatypes-perl_6.02-1_all.deb ...\n",
            "Unpacking liblwp-mediatypes-perl (6.02-1) ...\n",
            "Selecting previously unselected package libhttp-message-perl.\n",
            "Preparing to unpack .../12-libhttp-message-perl_6.14-1_all.deb ...\n",
            "Unpacking libhttp-message-perl (6.14-1) ...\n",
            "Selecting previously unselected package libhtml-form-perl.\n",
            "Preparing to unpack .../13-libhtml-form-perl_6.03-1_all.deb ...\n",
            "Unpacking libhtml-form-perl (6.03-1) ...\n",
            "Selecting previously unselected package libhtml-tree-perl.\n",
            "Preparing to unpack .../14-libhtml-tree-perl_5.07-1_all.deb ...\n",
            "Unpacking libhtml-tree-perl (5.07-1) ...\n",
            "Selecting previously unselected package libhtml-format-perl.\n",
            "Preparing to unpack .../15-libhtml-format-perl_2.12-1_all.deb ...\n",
            "Unpacking libhtml-format-perl (2.12-1) ...\n",
            "Selecting previously unselected package libhttp-cookies-perl.\n",
            "Preparing to unpack .../16-libhttp-cookies-perl_6.04-1_all.deb ...\n",
            "Unpacking libhttp-cookies-perl (6.04-1) ...\n",
            "Selecting previously unselected package libhttp-daemon-perl.\n",
            "Preparing to unpack .../17-libhttp-daemon-perl_6.01-1_all.deb ...\n",
            "Unpacking libhttp-daemon-perl (6.01-1) ...\n",
            "Selecting previously unselected package libhttp-negotiate-perl.\n",
            "Preparing to unpack .../18-libhttp-negotiate-perl_6.00-2_all.deb ...\n",
            "Unpacking libhttp-negotiate-perl (6.00-2) ...\n",
            "Selecting previously unselected package perl-openssl-defaults:amd64.\n",
            "Preparing to unpack .../19-perl-openssl-defaults_3build1_amd64.deb ...\n",
            "Unpacking perl-openssl-defaults:amd64 (3build1) ...\n",
            "Selecting previously unselected package libnet-ssleay-perl.\n",
            "Preparing to unpack .../20-libnet-ssleay-perl_1.84-1ubuntu0.2_amd64.deb ...\n",
            "Unpacking libnet-ssleay-perl (1.84-1ubuntu0.2) ...\n",
            "Selecting previously unselected package libio-socket-ssl-perl.\n",
            "Preparing to unpack .../21-libio-socket-ssl-perl_2.060-3~ubuntu18.04.1_all.deb ...\n",
            "Unpacking libio-socket-ssl-perl (2.060-3~ubuntu18.04.1) ...\n",
            "Selecting previously unselected package libnet-http-perl.\n",
            "Preparing to unpack .../22-libnet-http-perl_6.17-1_all.deb ...\n",
            "Unpacking libnet-http-perl (6.17-1) ...\n",
            "Selecting previously unselected package libtry-tiny-perl.\n",
            "Preparing to unpack .../23-libtry-tiny-perl_0.30-1_all.deb ...\n",
            "Unpacking libtry-tiny-perl (0.30-1) ...\n",
            "Selecting previously unselected package libwww-robotrules-perl.\n",
            "Preparing to unpack .../24-libwww-robotrules-perl_6.01-1_all.deb ...\n",
            "Unpacking libwww-robotrules-perl (6.01-1) ...\n",
            "Selecting previously unselected package libwww-perl.\n",
            "Preparing to unpack .../25-libwww-perl_6.31-1ubuntu0.1_all.deb ...\n",
            "Unpacking libwww-perl (6.31-1ubuntu0.1) ...\n",
            "Selecting previously unselected package liblwp-protocol-https-perl.\n",
            "Preparing to unpack .../26-liblwp-protocol-https-perl_6.07-2_all.deb ...\n",
            "Unpacking liblwp-protocol-https-perl (6.07-2) ...\n",
            "Selecting previously unselected package libnet-smtp-ssl-perl.\n",
            "Preparing to unpack .../27-libnet-smtp-ssl-perl_1.04-1_all.deb ...\n",
            "Unpacking libnet-smtp-ssl-perl (1.04-1) ...\n",
            "Selecting previously unselected package libmailtools-perl.\n",
            "Preparing to unpack .../28-libmailtools-perl_2.18-1_all.deb ...\n",
            "Unpacking libmailtools-perl (2.18-1) ...\n",
            "Selecting previously unselected package libxml-parser-perl.\n",
            "Preparing to unpack .../29-libxml-parser-perl_2.44-2build3_amd64.deb ...\n",
            "Unpacking libxml-parser-perl (2.44-2build3) ...\n",
            "Selecting previously unselected package libauthen-sasl-perl.\n",
            "Preparing to unpack .../30-libauthen-sasl-perl_2.1600-1_all.deb ...\n",
            "Unpacking libauthen-sasl-perl (2.1600-1) ...\n",
            "Setting up libhtml-tagset-perl (3.20-3) ...\n",
            "Setting up libtry-tiny-perl (0.30-1) ...\n",
            "Setting up libfont-afm-perl (1.20-2) ...\n",
            "Setting up libencode-locale-perl (1.05-1) ...\n",
            "Setting up libtimedate-perl (2.3000-2) ...\n",
            "Setting up perl-openssl-defaults:amd64 (3build1) ...\n",
            "Setting up libio-html-perl (1.001-1) ...\n",
            "Setting up liblwp-mediatypes-perl (6.02-1) ...\n",
            "Setting up liburi-perl (1.73-1) ...\n",
            "Setting up libdata-dump-perl (1.23-1) ...\n",
            "Setting up libhtml-parser-perl (3.72-3build1) ...\n",
            "Setting up libnet-http-perl (6.17-1) ...\n",
            "Setting up libwww-robotrules-perl (6.01-1) ...\n",
            "Setting up libauthen-sasl-perl (2.1600-1) ...\n",
            "Setting up netbase (5.4) ...\n",
            "Setting up libhttp-date-perl (6.02-1) ...\n",
            "Setting up libnet-ssleay-perl (1.84-1ubuntu0.2) ...\n",
            "Setting up libio-socket-ssl-perl (2.060-3~ubuntu18.04.1) ...\n",
            "Setting up libhtml-tree-perl (5.07-1) ...\n",
            "Setting up libfile-listing-perl (6.04-1) ...\n",
            "Setting up libhttp-message-perl (6.14-1) ...\n",
            "Setting up libhttp-negotiate-perl (6.00-2) ...\n",
            "Setting up libnet-smtp-ssl-perl (1.04-1) ...\n",
            "Setting up libhtml-format-perl (2.12-1) ...\n",
            "Setting up libhttp-cookies-perl (6.04-1) ...\n",
            "Setting up libhttp-daemon-perl (6.01-1) ...\n",
            "Setting up libhtml-form-perl (6.03-1) ...\n",
            "Setting up libmailtools-perl (2.18-1) ...\n",
            "Setting up liblwp-protocol-https-perl (6.07-2) ...\n",
            "Setting up libwww-perl (6.31-1ubuntu0.1) ...\n",
            "Setting up libxml-parser-perl (2.44-2build3) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "/content/pyrouge/pyrouge\n",
            "Cloning into 'rouge'...\n",
            "remote: Enumerating objects: 393, done.\u001b[K\n",
            "remote: Total 393 (delta 0), reused 0 (delta 0), pack-reused 393\u001b[K\n",
            "Receiving objects: 100% (393/393), 298.74 KiB | 5.64 MiB/s, done.\n",
            "Resolving deltas: 100% (109/109), done.\n",
            "2021-04-08 17:23:52,084 [MainThread  ] [INFO ]  Set ROUGE home directory to /content/pyrouge/rouge/tools/ROUGE-1.5.5.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/pyrouge_set_rouge_path\", line 4, in <module>\n",
            "    __import__('pkg_resources').run_script('pyrouge==0.1.3', 'pyrouge_set_rouge_path')\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pkg_resources/__init__.py\", line 651, in run_script\n",
            "    self.require(requires)[0].run_script(script_name, ns)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pkg_resources/__init__.py\", line 1448, in run_script\n",
            "    exec(code, namespace, namespace)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pyrouge-0.1.3-py3.7.egg/EGG-INFO/scripts/pyrouge_set_rouge_path\", line 18, in <module>\n",
            "    main()\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pyrouge-0.1.3-py3.7.egg/EGG-INFO/scripts/pyrouge_set_rouge_path\", line 15, in main\n",
            "    Rouge155(args.home_dir)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pyrouge-0.1.3-py3.7.egg/pyrouge/Rouge155.py\", line 91, in __init__\n",
            "    self.__set_rouge_dir(rouge_dir)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pyrouge-0.1.3-py3.7.egg/pyrouge/Rouge155.py\", line 413, in __set_rouge_dir\n",
            "    self.data_dir = os.path.join(self._home_dir, 'data')\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pyrouge-0.1.3-py3.7.egg/pyrouge/Rouge155.py\", line 549, in fset\n",
            "    verify_dir(path, dir_name)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pyrouge-0.1.3-py3.7.egg/pyrouge/utils/file_utils.py\", line 87, in verify_dir\n",
            "    raise Exception(msg)\n",
            "Exception: Cannot set data directory because the path /content/pyrouge/rouge/tools/ROUGE-1.5.5/data does not exist.\n",
            "[Errno 2] No such file or directory: '/content/pyrouge/rouge/tools/ROUGE-1.5.5/data'\n",
            "/content/pyrouge/pyrouge\n",
            "mv: cannot stat 'WordNet-2.0.exc.db': No such file or directory\n",
            "Can't open perl script \"WordNet-2.0-Exceptions/buildExeptionDB.pl\": No such file or directory\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fkQhs-DW_GK9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "de4f1001-90d2-431e-9f30-709ace46b522"
      },
      "source": [
        "# 기타 패키지 설치\n",
        "!pip install -q pytorch_pretrained_bert\n",
        "!pip install -q tensorboardX\n",
        "!pip install -q jupyter-dash==0.3.0rc1 dash-bootstrap-components transformers"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[K     |████████████████████████████████| 133kB 4.1MB/s \n",
            "\u001b[K     |████████████████████████████████| 133kB 5.7MB/s \n",
            "\u001b[K     |████████████████████████████████| 7.4MB 6.8MB/s \n",
            "\u001b[K     |████████████████████████████████| 81kB 6.2MB/s \n",
            "\u001b[31mERROR: botocore 1.20.47 has requirement urllib3<1.27,>=1.25.4, but you'll have urllib3 1.24.3 which is incompatible.\u001b[0m\n",
            "\u001b[K     |████████████████████████████████| 122kB 4.4MB/s \n",
            "\u001b[K     |████████████████████████████████| 51kB 2.2MB/s \n",
            "\u001b[K     |████████████████████████████████| 194kB 6.3MB/s \n",
            "\u001b[K     |████████████████████████████████| 2.2MB 7.4MB/s \n",
            "\u001b[K     |████████████████████████████████| 81kB 7.3MB/s \n",
            "\u001b[K     |████████████████████████████████| 3.3MB 16.9MB/s \n",
            "\u001b[K     |████████████████████████████████| 870kB 44.9MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.0MB 52.4MB/s \n",
            "\u001b[K     |████████████████████████████████| 3.5MB 39.2MB/s \n",
            "\u001b[K     |████████████████████████████████| 194kB 36.6MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.8MB 44.0MB/s \n",
            "\u001b[K     |████████████████████████████████| 358kB 39.8MB/s \n",
            "\u001b[?25h  Building wheel for dash (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for dash-renderer (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for dash-core-components (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for dash-html-components (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for dash-table (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Uuo_VZyYvbP"
      },
      "source": [
        "#Google Drive Mount"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gqy8CkL4qMNW",
        "outputId": "91ebf57c-b815-4e0c-af51-aee48f94479f"
      },
      "source": [
        "#gd mount\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eNHS_RluhGcv"
      },
      "source": [
        "#2. BERT Model download"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iy23A3EaQc97"
      },
      "source": [
        "import os\n",
        "os.chdir('/content')"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZJSyUZLagMYV",
        "outputId": "526423bb-753f-43f6-f9ec-8042b7907a85"
      },
      "source": [
        "#load fine-tuned bert model\n",
        "!gdown --id 1-ER4JzlMEBl7GY5G7K_FqCe5jLyjvQ2w"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1-ER4JzlMEBl7GY5G7K_FqCe5jLyjvQ2w\n",
            "To: /content/model_step_1000.pt\n",
            "1.31GB [00:11, 114MB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fnShxFgZqV_f"
      },
      "source": [
        "# data에 저장된 etri-bert 모델 가져와서 압축해제\n",
        "!cp \"/content/gdrive/My Drive/data/1_bert_download_001_bert_morp_pytorch.zip\" \"1_bert_download_001_bert_morp_pytorch.zip\"\n",
        "!unzip -q \"1_bert_download_001_bert_morp_pytorch.zip\""
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jntLwHwKbI0p"
      },
      "source": [
        "#KoBertSum package\n",
        "!git clone -q https://github.com/raqoon886/KoBertSum.git"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wv5f3XOLpZ7o"
      },
      "source": [
        "# Bertsum directory chdir\n",
        "os.chdir('/content/KoBertSum/src')"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d3Uzgqx3cLy0"
      },
      "source": [
        "#3. BERT forward propagation workflow"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M3m9X2IApKrt"
      },
      "source": [
        "\"\"\"\n",
        "    Main training workflow\n",
        "\"\"\"\n",
        "from __future__ import division\n",
        "\n",
        "import argparse\n",
        "import glob\n",
        "import os\n",
        "import random\n",
        "import signal\n",
        "import time\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "from pytorch_pretrained_bert import BertConfig\n",
        "\n",
        "\n",
        "import distributed\n",
        "from models import data_loader, model_builder\n",
        "from models.data_loader import load_dataset\n",
        "from models.model_builder import Summarizer\n",
        "from tensorboardX import SummaryWriter\n",
        "from models.reporter import ReportMgr\n",
        "from models.stats import Statistics\n",
        "from others.logging import logger\n",
        "# from models.trainer import build_trainer\n",
        "# build_trainer의 dependency package pyrouge.utils가 import되지 않아 직접 셀에 삽입\n",
        "from others.logging import logger, init_logger\n",
        "\n",
        "def build_trainer(args, device_id, model,\n",
        "                  optim):\n",
        "    \"\"\"\n",
        "    Simplify `Trainer` creation based on user `opt`s*\n",
        "    Args:\n",
        "        opt (:obj:`Namespace`): user options (usually from argument parsing)\n",
        "        model (:obj:`onmt.models.NMTModel`): the model to train\n",
        "        fields (dict): dict of fields\n",
        "        optim (:obj:`onmt.utils.Optimizer`): optimizer used during training\n",
        "        data_type (str): string describing the type of data\n",
        "            e.g. \"text\", \"img\", \"audio\"\n",
        "        model_saver(:obj:`onmt.models.ModelSaverBase`): the utility object\n",
        "            used to save the model\n",
        "    \"\"\"\n",
        "    device = \"cpu\" if args.visible_gpus == '-1' else \"cuda\"\n",
        "\n",
        "\n",
        "    grad_accum_count = args.accum_count\n",
        "    n_gpu = args.world_size\n",
        "\n",
        "    if device_id >= 0:\n",
        "        gpu_rank = int(args.gpu_ranks[device_id])\n",
        "    else:\n",
        "        gpu_rank = 0\n",
        "        n_gpu = 0\n",
        "\n",
        "    print('gpu_rank %d' % gpu_rank)\n",
        "\n",
        "    tensorboard_log_dir = args.model_path\n",
        "\n",
        "    writer = SummaryWriter(tensorboard_log_dir, comment=\"Unmt\")\n",
        "\n",
        "    report_manager = ReportMgr(args.report_every, start_time=-1, tensorboard_writer=writer)\n",
        "\n",
        "    trainer = Trainer(args, model, optim, grad_accum_count, n_gpu, gpu_rank, report_manager)\n",
        "\n",
        "    # print(tr)\n",
        "    if (model):\n",
        "        n_params = _tally_parameters(model)\n",
        "        logger.info('* number of parameters: %d' % n_params)\n",
        "\n",
        "    return trainer\n",
        "class Trainer(object):\n",
        "    \"\"\"\n",
        "    Class that controls the training process.\n",
        "\n",
        "    Args:\n",
        "            model(:py:class:`onmt.models.model.NMTModel`): translation model\n",
        "                to train\n",
        "            train_loss(:obj:`onmt.utils.loss.LossComputeBase`):\n",
        "               training loss computation\n",
        "            valid_loss(:obj:`onmt.utils.loss.LossComputeBase`):\n",
        "               training loss computation\n",
        "            optim(:obj:`onmt.utils.optimizers.Optimizer`):\n",
        "               the optimizer responsible for update\n",
        "            trunc_size(int): length of truncated back propagation through time\n",
        "            shard_size(int): compute loss in shards of this size for efficiency\n",
        "            data_type(string): type of the source input: [text|img|audio]\n",
        "            norm_method(string): normalization methods: [sents|tokens]\n",
        "            grad_accum_count(int): accumulate gradients this many times.\n",
        "            report_manager(:obj:`onmt.utils.ReportMgrBase`):\n",
        "                the object that creates reports, or None\n",
        "            model_saver(:obj:`onmt.models.ModelSaverBase`): the saver is\n",
        "                used to save a checkpoint.\n",
        "                Thus nothing will be saved if this parameter is None\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self,  args, model,  optim,\n",
        "                  grad_accum_count=1, n_gpu=1, gpu_rank=1,\n",
        "                  report_manager=None):\n",
        "        # Basic attributes.\n",
        "        self.args = args\n",
        "        self.save_checkpoint_steps = args.save_checkpoint_steps\n",
        "        self.model = model\n",
        "        self.optim = optim\n",
        "        self.grad_accum_count = grad_accum_count\n",
        "        self.n_gpu = n_gpu\n",
        "        self.gpu_rank = gpu_rank\n",
        "        self.report_manager = report_manager\n",
        "\n",
        "        self.loss = torch.nn.BCELoss(reduction='none')\n",
        "        assert grad_accum_count > 0\n",
        "        # Set model in training mode.\n",
        "        if (model):\n",
        "            self.model.train()\n",
        "\n",
        "    def summary(self, test_iter, step, cal_lead=False, cal_oracle=False):\n",
        "        \"\"\" Validate model.\n",
        "            valid_iter: validate data iterator\n",
        "        Returns:\n",
        "            :obj:`nmt.Statistics`: validation loss statistics\n",
        "        \"\"\"\n",
        "        # Set model in validating mode.\n",
        "        def _get_ngrams(n, text):\n",
        "            ngram_set = set()\n",
        "            text_length = len(text)\n",
        "            max_index_ngram_start = text_length - n\n",
        "            for i in range(max_index_ngram_start + 1):\n",
        "                ngram_set.add(tuple(text[i:i + n]))\n",
        "            return ngram_set\n",
        "\n",
        "        def _block_tri(c, p):\n",
        "            tri_c = _get_ngrams(3, c.split())\n",
        "            for s in p:\n",
        "                tri_s = _get_ngrams(3, s.split())\n",
        "                if len(tri_c.intersection(tri_s))>0:\n",
        "                    return True\n",
        "            return False\n",
        "\n",
        "        if (not cal_lead and not cal_oracle):\n",
        "            self.model.eval()\n",
        "        stats = Statistics()\n",
        "\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            for batch in test_iter:\n",
        "                src = batch.src\n",
        "                labels = batch.labels\n",
        "                segs = batch.segs\n",
        "                clss = batch.clss\n",
        "                mask = batch.mask\n",
        "                mask_cls = batch.mask_cls\n",
        "\n",
        "\n",
        "                gold = []\n",
        "                pred = []\n",
        "\n",
        "                if (cal_lead):\n",
        "                    selected_ids = [list(range(batch.clss.size(1)))] * batch.batch_size\n",
        "                elif (cal_oracle):\n",
        "                    selected_ids = [[j for j in range(batch.clss.size(1)) if labels[i][j] == 1] for i in\n",
        "                                    range(batch.batch_size)]\n",
        "                else:\n",
        "                    sent_scores, mask = self.model(src, segs, clss, mask, mask_cls)\n",
        "\n",
        "                    # loss = self.loss(sent_scores, labels.float())\n",
        "                    # loss = (loss * mask.float()).sum()\n",
        "                    # batch_stats = Statistics(float(loss.cpu().data.numpy()), len(labels))\n",
        "                    # stats.update(batch_stats)\n",
        "\n",
        "                    sent_scores = sent_scores + mask.float()\n",
        "                    sent_scores = sent_scores.cpu().data.numpy()\n",
        "                    selected_ids = np.argsort(-sent_scores, 1)\n",
        "                # selected_ids = np.sort(selected_ids,1)\n",
        "                \n",
        "\n",
        "        return selected_ids\n",
        "\n",
        "\n",
        "    def _gradient_accumulation(self, true_batchs, normalization, total_stats,\n",
        "                               report_stats):\n",
        "        if self.grad_accum_count > 1:\n",
        "            self.model.zero_grad()\n",
        "\n",
        "        for batch in true_batchs:\n",
        "            if self.grad_accum_count == 1:\n",
        "                self.model.zero_grad()\n",
        "\n",
        "            src = batch.src\n",
        "            labels = batch.labels\n",
        "            segs = batch.segs\n",
        "            clss = batch.clss\n",
        "            mask = batch.mask\n",
        "            mask_cls = batch.mask_cls\n",
        "\n",
        "            sent_scores, mask = self.model(src, segs, clss, mask, mask_cls)\n",
        "\n",
        "            loss = self.loss(sent_scores, labels.float())\n",
        "            loss = (loss*mask.float()).sum()\n",
        "            (loss/loss.numel()).backward()\n",
        "            # loss.div(float(normalization)).backward()\n",
        "\n",
        "            batch_stats = Statistics(float(loss.cpu().data.numpy()), normalization)\n",
        "\n",
        "\n",
        "            total_stats.update(batch_stats)\n",
        "            report_stats.update(batch_stats)\n",
        "\n",
        "            # 4. Update the parameters and statistics.\n",
        "            if self.grad_accum_count == 1:\n",
        "                # Multi GPU gradient gather\n",
        "                if self.n_gpu > 1:\n",
        "                    grads = [p.grad.data for p in self.model.parameters()\n",
        "                             if p.requires_grad\n",
        "                             and p.grad is not None]\n",
        "                    distributed.all_reduce_and_rescale_tensors(\n",
        "                        grads, float(1))\n",
        "                self.optim.step()\n",
        "\n",
        "        # in case of multi step gradient accumulation,\n",
        "        # update only after accum batches\n",
        "        if self.grad_accum_count > 1:\n",
        "            if self.n_gpu > 1:\n",
        "                grads = [p.grad.data for p in self.model.parameters()\n",
        "                         if p.requires_grad\n",
        "                         and p.grad is not None]\n",
        "                distributed.all_reduce_and_rescale_tensors(\n",
        "                    grads, float(1))\n",
        "            self.optim.step()\n",
        "\n",
        "    def _save(self, step):\n",
        "        real_model = self.model\n",
        "        # real_generator = (self.generator.module\n",
        "        #                   if isinstance(self.generator, torch.nn.DataParallel)\n",
        "        #                   else self.generator)\n",
        "\n",
        "        model_state_dict = real_model.state_dict()\n",
        "        # generator_state_dict = real_generator.state_dict()\n",
        "        checkpoint = {\n",
        "            'model': model_state_dict,\n",
        "            # 'generator': generator_state_dict,\n",
        "            'opt': self.args,\n",
        "            'optim': self.optim,\n",
        "        }\n",
        "        checkpoint_path = os.path.join(self.args.model_path, 'model_step_%d.pt' % step)\n",
        "        logger.info(\"Saving checkpoint %s\" % checkpoint_path)\n",
        "        # checkpoint_path = '%s_step_%d.pt' % (FLAGS.model_path, step)\n",
        "        if (not os.path.exists(checkpoint_path)):\n",
        "            torch.save(checkpoint, checkpoint_path)\n",
        "            return checkpoint, checkpoint_path\n",
        "\n",
        "    def _start_report_manager(self, start_time=None):\n",
        "        \"\"\"\n",
        "        Simple function to start report manager (if any)\n",
        "        \"\"\"\n",
        "        if self.report_manager is not None:\n",
        "            if start_time is None:\n",
        "                self.report_manager.start()\n",
        "            else:\n",
        "                self.report_manager.start_time = start_time\n",
        "\n",
        "    def _maybe_gather_stats(self, stat):\n",
        "        \"\"\"\n",
        "        Gather statistics in multi-processes cases\n",
        "\n",
        "        Args:\n",
        "            stat(:obj:onmt.utils.Statistics): a Statistics object to gather\n",
        "                or None (it returns None in this case)\n",
        "\n",
        "        Returns:\n",
        "            stat: the updated (or unchanged) stat object\n",
        "        \"\"\"\n",
        "        if stat is not None and self.n_gpu > 1:\n",
        "            return Statistics.all_gather_stats(stat)\n",
        "        return stat\n",
        "\n",
        "    def _maybe_report_training(self, step, num_steps, learning_rate,\n",
        "                               report_stats):\n",
        "        \"\"\"\n",
        "        Simple function to report training stats (if report_manager is set)\n",
        "        see `onmt.utils.ReportManagerBase.report_training` for doc\n",
        "        \"\"\"\n",
        "        if self.report_manager is not None:\n",
        "            return self.report_manager.report_training(\n",
        "                step, num_steps, learning_rate, report_stats,\n",
        "                multigpu=self.n_gpu > 1)\n",
        "\n",
        "    def _report_step(self, learning_rate, step, train_stats=None,\n",
        "                     valid_stats=None):\n",
        "        \"\"\"\n",
        "        Simple function to report stats (if report_manager is set)\n",
        "        see `onmt.utils.ReportManagerBase.report_step` for doc\n",
        "        \"\"\"\n",
        "        if self.report_manager is not None:\n",
        "            return self.report_manager.report_step(\n",
        "                learning_rate, step, train_stats=train_stats,\n",
        "                valid_stats=valid_stats)\n",
        "\n",
        "    def _maybe_save(self, step):\n",
        "        \"\"\"\n",
        "        Save the model if a model saver is set\n",
        "        \"\"\"\n",
        "        if self.model_saver is not None:\n",
        "            self.model_saver.maybe_save(step)\n",
        "\n",
        "import easydict\n",
        "args = easydict.EasyDict({\n",
        "    \"encoder\":'classifier',\n",
        "    \"mode\":'summary',\n",
        "    \"bert_data_path\":'/content/bert_sample/korean',\n",
        "    \"model_path\":'../models/bert_classifier',\n",
        "    \"bert_model\":'/content/001_bert_morp_pytorch',\n",
        "    \"result_path\":'../results/korean',\n",
        "    \"temp_dir\":'.',\n",
        "    \"bert_config_path\":'/content/001_bert_morp_pytorch/bert_config.json',\n",
        "    \"batch_size\":1000,\n",
        "    \"use_interval\":True,\n",
        "    \"hidden_size\":128,\n",
        "    \"ff_size\":512,\n",
        "    \"heads\":4,\n",
        "    \"inter_layers\":2,\n",
        "    \"rnn_size\":512,\n",
        "    \"param_init\":0,\n",
        "    \"param_init_glorot\":True,\n",
        "    \"dropout\":0.1,\n",
        "    \"optim\":'adam',\n",
        "    \"lr\":2e-3,\n",
        "    \"report_every\":1,\n",
        "    \"save_checkpoint_steps\":5,\n",
        "    \"block_trigram\":True,\n",
        "    \"recall_eval\":False,\n",
        "    \n",
        "    \"accum_count\":1,\n",
        "    \"world_size\":1,\n",
        "    \"visible_gpus\":'-1',\n",
        "    \"gpu_ranks\":'0',\n",
        "    \"log_file\":'../logs/bert_classifier',\n",
        "    \"test_from\":'/content/model_step_1000.pt'\n",
        "})\n",
        "args.gpu_ranks = [int(i) for i in args.gpu_ranks.split(',')]\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = args.visible_gpus\n",
        "\n",
        "init_logger(args.log_file)\n",
        "device = \"cpu\" if args.visible_gpus == '-1' else \"cuda\"\n",
        "device_id = 0 if device == \"cuda\" else -1\n",
        "model_flags = ['hidden_size', 'ff_size', 'heads', 'inter_layers','encoder','ff_actv', 'use_interval','rnn_size']\n",
        "\n",
        "def summary(args, b_list, device_id, pt, step):\n",
        "\n",
        "    device = \"cpu\" if args.visible_gpus == '-1' else \"cuda\"\n",
        "    if (pt != ''):\n",
        "        test_from = pt\n",
        "    else:\n",
        "        test_from = args.test_from\n",
        "    logger.info('Loading checkpoint from %s' % test_from)\n",
        "    checkpoint = torch.load(test_from, map_location=lambda storage, loc: storage)\n",
        "    opt = vars(checkpoint['opt'])\n",
        "    for k in opt.keys():\n",
        "        if (k in model_flags):\n",
        "            setattr(args, k, opt[k])\n",
        "    print(args)\n",
        "\n",
        "    config = BertConfig.from_json_file(args.bert_config_path)\n",
        "    model = Summarizer(args, device, load_pretrained_bert=False, bert_config = config)\n",
        "    model.load_cp(checkpoint)\n",
        "    model.eval()\n",
        "\n",
        "    test_iter =data_loader.Dataloader(args, _lazy_dataset_loader(b_list),\n",
        "                                  args.batch_size, device,\n",
        "                                  shuffle=False, is_test=True)\n",
        "    trainer = build_trainer(args, device_id, model, None)\n",
        "    result = trainer.summary(test_iter,step)\n",
        "    return result\n",
        "def _tally_parameters(model):\n",
        "    n_params = sum([p.nelement() for p in model.parameters()])\n",
        "    return n_params"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xji5mhCxcWcg"
      },
      "source": [
        "#4. Input data morp-tokenization workflow"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Diau6__Vhhuo"
      },
      "source": [
        "##your openapi_key"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "moouXLWzclio"
      },
      "source": [
        "openapi_key = ''"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ifDaOjcThpEr"
      },
      "source": [
        "##workflow"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UAnJ4R8b0hYd"
      },
      "source": [
        "import argparse\n",
        "import json\n",
        "import os\n",
        "import time\n",
        "import urllib3\n",
        "from glob import glob\n",
        "import collections\n",
        "import six\n",
        "import gc\n",
        "\n",
        "def do_lang ( openapi_key, text ) :\n",
        "    openApiURL = \"http://aiopen.etri.re.kr:8000/WiseNLU\"\n",
        "    requestJson = { \"access_key\": openapi_key, \"argument\": { \"text\": text, \"analysis_code\": \"morp\" } }\n",
        "    http = urllib3.PoolManager()\n",
        "    response = http.request( \"POST\", openApiURL, headers={\"Content-Type\": \"application/json; charset=UTF-8\"}, body=json.dumps(requestJson))\n",
        "    \n",
        "    json_data = json.loads(response.data.decode('utf-8'))\n",
        "    json_result = json_data[\"result\"]\n",
        "    \n",
        "    if json_result == -1:\n",
        "        json_reason = json_data[\"reason\"]\n",
        "        if \"Invalid Access Key\" in json_reason:\n",
        "            logger.info(json_reason)\n",
        "            logger.info(\"Please check the openapi access key.\")\n",
        "            sys.exit()\n",
        "        return \"openapi error - \" + json_reason\n",
        "    else:\n",
        "        json_data = json.loads(response.data.decode('utf-8'))\n",
        "    \n",
        "        json_return_obj = json_data[\"return_object\"]\n",
        "        \n",
        "        return_result = \"\"\n",
        "        json_sentence = json_return_obj[\"sentence\"]\n",
        "        for json_morp in json_sentence:\n",
        "            for morp in json_morp[\"morp\"]:\n",
        "                return_result = return_result+str(morp[\"lemma\"])+\"/\"+str(morp[\"type\"])+\" \"\n",
        "\n",
        "        return return_result\n",
        "class BertData():\n",
        "    def __init__(self, vocab_file_path):\n",
        "        self.tokenizer = Tokenizer(vocab_file_path)\n",
        "        self.sep_vid = self.tokenizer.vocab['[SEP]']\n",
        "        self.cls_vid = self.tokenizer.vocab['[CLS]']\n",
        "        self.pad_vid = self.tokenizer.vocab['[PAD]']\n",
        "\n",
        "    def preprocess(self, src):\n",
        "\n",
        "        if (len(src) == 0):\n",
        "            return None\n",
        "\n",
        "        original_src_txt = [''.join(s) for s in src]\n",
        "\n",
        "\n",
        "        idxs = [i for i, s in enumerate(src) if (len(s) > 0)]\n",
        "\n",
        "        src = [src[i][:20000] for i in idxs]\n",
        "        src = src[:10000]\n",
        "\n",
        "        if (len(src) < 3):\n",
        "            return None\n",
        "\n",
        "        src_txt = [''.join(sent) for sent in src]\n",
        "        text = ' [SEP] [CLS] '.join(src_txt)\n",
        "        src_subtokens = text.split(' ')\n",
        "        src_subtokens = src_subtokens[:510]\n",
        "        src_subtokens = ['[CLS]'] + src_subtokens + ['[SEP]']\n",
        "\n",
        "        src_subtoken_idxs = self.tokenizer.convert_tokens_to_ids(src_subtokens)\n",
        "        _segs = [-1] + [i for i, t in enumerate(src_subtoken_idxs) if t == self.sep_vid]\n",
        "        segs = [_segs[i] - _segs[i - 1] for i in range(1, len(_segs))]\n",
        "        segments_ids = []\n",
        "        for i, s in enumerate(segs):\n",
        "            if (i % 2 == 0):\n",
        "                segments_ids += s * [0]\n",
        "            else:\n",
        "                segments_ids += s * [1]\n",
        "        cls_ids = [i for i, t in enumerate(src_subtoken_idxs) if t == self.cls_vid]\n",
        "        labels = None\n",
        "        tgt_txt = None\n",
        "        src_txt = [original_src_txt[i] for i in idxs]\n",
        "        return src_subtoken_idxs, labels, segments_ids, cls_ids, src_txt, tgt_txt\n",
        "def convert_to_unicode(text):\n",
        "    \"\"\"Converts `text` to Unicode (if it's not already), assuming utf-8 input.\"\"\"\n",
        "    if six.PY3:\n",
        "        if isinstance(text, str):\n",
        "            return text\n",
        "        elif isinstance(text, bytes):\n",
        "            return text.decode(\"utf-8\", \"ignore\")\n",
        "        else:\n",
        "            raise ValueError(\"Unsupported string type: %s\" % (type(text)))\n",
        "    elif six.PY2:\n",
        "        if isinstance(text, str):\n",
        "            return text.decode(\"utf-8\", \"ignore\")\n",
        "        elif isinstance(text, unicode):\n",
        "            return text\n",
        "        else:\n",
        "            raise ValueError(\"Unsupported string type: %s\" % (type(text)))\n",
        "    else:\n",
        "        raise ValueError(\"Not running on Python2 or Python 3?\")\n",
        "class Tokenizer(object):\n",
        "    def __init__(self, vocab_file_path):\n",
        "        self.vocab_file_path = vocab_file_path\n",
        "        \"\"\"Loads a vocabulary file into a dictionary.\"\"\"\n",
        "        vocab = collections.OrderedDict()\n",
        "        index = 0\n",
        "        with open(self.vocab_file_path, \"r\", encoding='utf-8') as reader:\n",
        "\n",
        "            while True:\n",
        "                token = convert_to_unicode(reader.readline())\n",
        "                if not token:\n",
        "                    break\n",
        "\n",
        "          ### joonho.lim @ 2019-03-15\n",
        "                if token.find('n_iters=') == 0 or token.find('max_length=') == 0 :\n",
        "\n",
        "                    continue\n",
        "                token = token.split('\\t')[0].strip('_')\n",
        "\n",
        "                token = token.strip()\n",
        "                vocab[token] = index\n",
        "                index += 1\n",
        "        self.vocab = vocab\n",
        "    def convert_tokens_to_ids(self, tokens):\n",
        "        \"\"\"Converts a sequence of tokens into ids using the vocab.\"\"\"\n",
        "        ids = []\n",
        "        for token in tokens:\n",
        "            try:\n",
        "                ids.append(self.vocab[token])\n",
        "            except:\n",
        "                ids.append(1)\n",
        "        if len(ids) > 10000:\n",
        "            raise ValueError(\n",
        "                \"Token indices sequence length is longer than the specified maximum \"\n",
        "                \" sequence length for this BERT model ({} > {}). Running this\"\n",
        "                \" sequence through BERT will result in indexing errors\".format(len(ids), 10000)\n",
        "            )\n",
        "        return ids\n",
        "def _lazy_dataset_loader(pt_file):\n",
        "    \n",
        "    dataset = pt_file\n",
        "    \n",
        "    yield dataset\n",
        "def News_to_input(text, openapi_key):\n",
        "    newstemp = do_lang(openapi_key,text)\n",
        "    news = newstemp.split(' ./SF ')[:-1]\n",
        "    bertdata = BertData('/content/001_bert_morp_pytorch/vocab.korean_morp.list')\n",
        "    tmp = bertdata.preprocess(news)\n",
        "    b_data_dict = {\"src\":tmp[0],\n",
        "               \"labels\":[0,1,2],\n",
        "               \"segs\":tmp[2],\n",
        "               \"clss\":tmp[3],\n",
        "               \"src_txt\":tmp[4],\n",
        "               \"tgt_txt\":'hehe'}\n",
        "    b_list = []\n",
        "    b_list.append(b_data_dict) \n",
        "    return b_list"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6nhubPG1c5zt"
      },
      "source": [
        "#5. html for SummaryBot"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U0HisA9GdU95"
      },
      "source": [
        "import time\n",
        "\n",
        "import dash\n",
        "import dash_html_components as html\n",
        "import dash_core_components as dcc\n",
        "import dash_bootstrap_components as dbc\n",
        "from dash.dependencies import Input, Output, State\n",
        "from jupyter_dash import JupyterDash\n",
        "from transformers import AutoModelWithLMHead, AutoTokenizer\n",
        "import torch\n",
        "def textbox(text, box=\"other\"):\n",
        "    style = {\n",
        "        \"max-width\": \"55%\",\n",
        "        \"width\": \"max-content\",\n",
        "        \"padding\": \"10px 15px\",\n",
        "        \"border-radius\": \"25px\"\n",
        "    }\n",
        "\n",
        "    if box == \"self\":\n",
        "        style[\"margin-left\"] = \"auto\"\n",
        "        style[\"margin-right\"] = 0\n",
        "\n",
        "        color = \"primary\"\n",
        "        inverse = True\n",
        "\n",
        "    elif box == \"other\":\n",
        "        style[\"margin-left\"] = 0\n",
        "        style[\"margin-right\"] = \"auto\"\n",
        "\n",
        "        color = \"light\"\n",
        "        inverse = False\n",
        "\n",
        "    else:\n",
        "        raise ValueError(\"Incorrect option for `box`.\")\n",
        "\n",
        "    return dbc.Card(text, style=style, body=True, color=color, inverse=inverse)\n",
        "conversation = html.Div(\n",
        "    style={\n",
        "        \"width\": \"80%\",\n",
        "        \"max-width\": \"800px\",\n",
        "        \"height\": \"70vh\",\n",
        "        \"margin\": \"auto\",\n",
        "        \"overflow-y\": \"auto\",\n",
        "    },\n",
        "    id=\"display-conversation\",\n",
        ")\n",
        "\n",
        "controls = dbc.InputGroup(\n",
        "    style={\"width\": \"80%\", \"max-width\": \"800px\", \"margin\": \"auto\"},\n",
        "    children=[\n",
        "        dbc.Input(id=\"user-input\", placeholder=\"Write to the chatbot...\", type=\"text\"),\n",
        "        dbc.InputGroupAddon(dbc.Button(\"Submit\", id=\"submit\"), addon_type=\"append\",),\n",
        "    ],\n",
        ")\n",
        "\n",
        "\n",
        "# Define app\n",
        "app = JupyterDash(__name__, external_stylesheets=[dbc.themes.BOOTSTRAP])\n",
        "server = app.server\n",
        "\n",
        "\n",
        "# Define Layout\n",
        "app.layout = dbc.Container(\n",
        "    fluid=True,\n",
        "    style={'background-image': 'url(https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=http%3A%2F%2Fcfile21.uf.tistory.com%2Fimage%2F99A30D4B5CB15385210EA0)'},\n",
        "    children=[\n",
        "        html.H1(\"뉴스뚝딱\"),\n",
        "        html.Hr(),\n",
        "        dcc.Store(id=\"store-conversation\", data=\"\"),\n",
        "        conversation,\n",
        "        controls\n",
        "    ],\n",
        ")\n",
        "@app.callback(\n",
        "    Output(\"display-conversation\", \"children\"), [Input(\"store-conversation\", \"data\")]\n",
        ")\n",
        "def update_display(chat_history):\n",
        "    return [\n",
        "        textbox(x, box=\"self\") if i % 2 == 0 else textbox(x, box=\"other\")\n",
        "        for i, x in enumerate(chat_history.split('<token>'))\n",
        "    ]\n",
        "\n",
        "\n",
        "@app.callback(\n",
        "    [Output(\"store-conversation\", \"data\"), Output(\"user-input\", \"value\")],\n",
        "    [Input(\"submit\", \"n_clicks\"), Input(\"user-input\", \"n_submit\")],\n",
        "    [State(\"user-input\", \"value\"), State(\"store-conversation\", \"data\")],\n",
        ")\n",
        "def run_chatbot(n_clicks, n_submit, user_input, chat_history):\n",
        "    if n_clicks == 0:\n",
        "        return \"\", \"\"\n",
        "\n",
        "    if user_input is None or user_input == \"\":\n",
        "        return chat_history, \"\"\n",
        "\n",
        "    # # temporary\n",
        "    # return chat_history + user_input + \"<|endoftext|>\" + user_input + \"<|endoftext|>\", \"\"\n",
        "\n",
        "    # encode the new user input, add the eos_token and return a tensor in Pytorch\n",
        "    bot_input_ids = News_to_input(chat_history + user_input, openapi_key)\n",
        "        \n",
        "    # generated a response while limiting the total chat history to 1000 tokens,\n",
        "    chat_history_ids = summary(args, bot_input_ids, -1, '', None)\n",
        "    pred_lst = list(chat_history_ids[0][:3])\n",
        "    final_text = ''\n",
        "    for i,a in enumerate(user_input.split('. ')):\n",
        "        if i in pred_lst:\n",
        "            final_text = final_text+a+'. '\n",
        "    chat_history = user_input + '<token>' +final_text\n",
        "\n",
        "    return chat_history, \"\""
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UW0GrA7Mg5kr"
      },
      "source": [
        "#6. RUN!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "id": "i_dGcr31dBdp",
        "outputId": "3bd742a8-1b65-4cff-bcd0-10816ab1caf7"
      },
      "source": [
        "app.run_server(mode='external')"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Dash app running on:\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "(async (port, path, text, element) => {\n",
              "    if (!google.colab.kernel.accessAllowed) {\n",
              "      return;\n",
              "    }\n",
              "    element.appendChild(document.createTextNode(''));\n",
              "    const url = await google.colab.kernel.proxyPort(port);\n",
              "    const anchor = document.createElement('a');\n",
              "    anchor.href = url + path;\n",
              "    anchor.target = '_blank';\n",
              "    anchor.setAttribute('data-href', url + path);\n",
              "    anchor.textContent = text;\n",
              "    element.appendChild(anchor);\n",
              "  })(8050, \"/\", \"http://127.0.0.1:8050/\", window.element)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}